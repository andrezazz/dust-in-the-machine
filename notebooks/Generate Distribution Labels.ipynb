{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from tqdm import notebook # Library for displaying progress bar\n",
    "from distfit import distfit\n",
    "\n",
    "bin_count = 151\n",
    "\n",
    "def get_dist_values(bins):\n",
    "    # Initialize distfit to fit the gen extreme distribution\n",
    "    dist = distfit(distr='genextreme')\n",
    "    \n",
    "    # Scale our data to our bins\n",
    "    t = np.floor(np.multiply(bins,bin_count))\n",
    "    new_samp = []\n",
    "    for idx, e in enumerate(t):\n",
    "        if e:\n",
    "            # To generate our samples add the bin number e number of times\n",
    "            new_samp.extend([idx for _  in range(int(e))])\n",
    "    # Fit the distribution to our data\n",
    "    if len(new_samp) == 0:\n",
    "        genextreme_c, genextreme_loc, genextreme_scale = 0\n",
    "    else:\n",
    "        dist.fit_transform(np.array(new_samp), verbose=1)\n",
    "        # Extract the model parameters\n",
    "        genextreme_c, genextreme_loc, genextreme_scale = dist.model['params']\n",
    "        \n",
    "    return [genextreme_c, genextreme_loc, genextreme_scale]\n",
    "\n",
    "def process_sample(i, snapshot_count, rhod, time, num_bins=151):\n",
    "    \"\"\" Creates a training sample from two points in time. Selects a random output bin for y, and saves the output bins for comparison\"\"\"\n",
    "    # First sample will always be the first and last element\n",
    "    if i == 0:\n",
    "        idxs = [0, snapshot_count-1]\n",
    "    else:\n",
    "        # Pick two indexes for snapshots (lowest = input, highest = output)\n",
    "        idxs = sorted([random.randint(0,snapshot_count-1) for _ in range(2)])\n",
    "    input_a = rhod[idxs[0]]\n",
    "    output_a = rhod[idxs[1]]\n",
    "    \n",
    "    new_input_bins = []\n",
    "    new_output_bins = []\n",
    "    input_bin_sum = np.sum(input_a)\n",
    "    output_bin_sum = np.sum(output_a)\n",
    "    for i in range(len(input_a)):\n",
    "            \n",
    "        # Get the old bins and sum them together to create the new one\n",
    "        # Also normalize the input bins\n",
    "        # Could add a statement here to leave out one of the input bins\n",
    "        new_input_bin = input_a[i] / input_bin_sum\n",
    "        if new_input_bin < 1e-30:\n",
    "            new_input_bin = 0\n",
    "        new_input_bins.append(new_input_bin)\n",
    "        \n",
    "        # Normalize the output bin so we can compare the prob distribution to it\n",
    "        new_output_bin = output_a[i] / output_bin_sum\n",
    "        if new_output_bin < 1e-30:\n",
    "            new_output_bin = 0\n",
    "        new_output_bins.append(new_output_bin)\n",
    "\n",
    "    output_dist_values = get_dist_values(new_output_bins)\n",
    "\n",
    "    # Time of the input\n",
    "    t = time[idxs[0]]\n",
    "        \n",
    "    # Difference of time in seconds between two snapshots\n",
    "    delta_t = time[idxs[1]] - t\n",
    "    \n",
    "    row = np.concatenate([input_params, new_input_bins, [t, delta_t], new_output_bins, output_dist_values])\n",
    "    return row\n",
    "\n",
    "def write_to_file(data, header=True, batch=False):\n",
    "    \"\"\" Helper method to write training data to a file\"\"\"\n",
    "    columns = ['R', 'Mstar', 'alpha', 'd2g', 'sigma', 'Tgas'] + [f'Input_Bin_{i}' for i in range(bin_count)]+ ['t','Delta_t'] + [f'Output_Bin_{i}' for i in range(bin_count)] + ['Output_genextremec_c', 'Output_genextreme_loc', 'Output_genextreme_scale']\n",
    "    df = pd.DataFrame(res, columns=columns)\n",
    "\n",
    "    # If writing in batch set the file mode to append\n",
    "    mode = 'a' if batch else 'w'\n",
    "    df.to_csv(filename, chunksize=100000, mode=mode, header=header, index=False)\n",
    "    \n",
    "filename = '/scratch/keh4nb/dust_training_data_all_bins_genextreme_full.csv'\n",
    "root_data_path = \"/project/SDS-capstones-kropko21/uva-astronomy/data/dust_coag_data_v1\"\n",
    "data_group = \"combined_v1\"\n",
    "\n",
    "# Store formatted data for training\n",
    "res = []\n",
    "\n",
    "chunk_size = 1000\n",
    "# Set this to a smaller number to get a smaller training set\n",
    "model_count = 10000\n",
    "writes = 0\n",
    "for d in notebook.tqdm(range(model_count)):\n",
    "    data_set = data_set = str(d).zfill(5)\n",
    "\n",
    "    data_dir = f\"{root_data_path}/{data_group}/data_{data_set}\"\n",
    "\n",
    "    input_params = None\n",
    "    # Open and extract the input parameters\n",
    "    with open(os.path.join(root_data_path, \"model_dict_v1.json\")) as f:\n",
    "        model_dict = json.load(f)\n",
    "        input_dict = model_dict[data_set]\n",
    "        input_params = [input_dict['R'], input_dict['Mstar'], input_dict['alpha'],input_dict['d2g'], input_dict['sigma'], input_dict['Tgas']]\n",
    "\n",
    "    try:\n",
    "        # `rho_dat`: The dust mass density (in g/cm^3) in each particle size/bin at a given snapshot in time. This is the main \"output\", i.e., the primary result, of any given model.\n",
    "        rhod = np.loadtxt(os.path.join(data_dir,\"rho_d.dat\"))\n",
    "        # Replace NaNs with 0s\n",
    "        rhod = np.nan_to_num(rhod)\n",
    "        # Replace negative values with 0s\n",
    "        rhod = np.where(rhod<0, 0, rhod) \n",
    "        \n",
    "        # `a_grid.dat`: The dust particle size in each \"bin\" in centimeters.\n",
    "        a_grid = np.loadtxt(os.path.join(data_dir, 'a_grid.dat'))\n",
    "\n",
    "        # `time.dat`: The time of each snapshot (in seconds).\n",
    "        time = np.loadtxt(os.path.join(data_dir, \"time.dat\"))\n",
    "    except Exception as e:\n",
    "        print(f'model {d} skipped')\n",
    "        import traceback\n",
    "        print(traceback.print_exc())\n",
    "        continue\n",
    "\n",
    "    snapshot_count = len(rhod)\n",
    "\n",
    "    # Set the number of samples\n",
    "    if snapshot_count > 15:\n",
    "        # Set the max to 100 for time as 15 cHr 2 is about 100\n",
    "        samples = 100\n",
    "    else:\n",
    "        # The number of pairs\n",
    "        samples = int(math.factorial(snapshot_count) / math.factorial(2) / math.factorial(snapshot_count-2))\n",
    "    \n",
    "    samples += 1\n",
    "    for i in range(samples):\n",
    "        row = process_sample(i, snapshot_count, rhod, time)\n",
    "        res.append(row)\n",
    "        \n",
    "    # Write to csv every x models to avoid oom\n",
    "    if d != 0 and d % chunk_size == (model_count - 1) % chunk_size:\n",
    "        writes += 1\n",
    "        # Only write the header on first chunk\n",
    "        header = writes == 1\n",
    "        write_to_file(res, header, batch=True)\n",
    "        res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from distfit import distfit\n",
    "from tqdm import tqdm\n",
    "\n",
    "bin_count = 151\n",
    "\n",
    "filename= \"/project/SDS-capstones-kropko21/uva-astronomy/dust_training_data_all_bins.csv\"\n",
    "data_set = pd.read_csv(filename)\n",
    "data_set_Y = data_set[[f'Output_Bin_{i}' for i in range(bin_count)]]\n",
    "\n",
    "dist = distfit(distr='popular')\n",
    "# Scale our data to our bins\n",
    "model_names = {}\n",
    "\n",
    "for idx, data in tqdm(data_set_Y.sample(100000).iterrows(), total=100000):\n",
    "    t = np.floor(np.multiply(data.values,bin_count))\n",
    "    new_samp = []\n",
    "    for idx, e in enumerate(t):\n",
    "        if e:\n",
    "            # To generate our samples add the bin number e number of times\n",
    "            new_samp.extend([idx for _  in range(int(e))])\n",
    "    # Fit the distribution to our data\n",
    "    if len(new_samp) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        dist.fit_transform(np.array(new_samp), verbose=1)\n",
    "        best_model = dist.model['name']\n",
    "        #print(dist.model['params'])\n",
    "        try:\n",
    "            model_names[best_model] += 1\n",
    "        except KeyError:\n",
    "            model_names[best_model] = 1\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
